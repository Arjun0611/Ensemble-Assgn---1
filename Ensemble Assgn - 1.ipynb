{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b586620-8425-4c28-889a-9b1d42f20cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1.\n",
    "\n",
    "# Ensemble Technique: Combining multiple machine learning models to create robust and accurate predictive model.\n",
    "\n",
    "# Aggregates predictions from diverse models to improve overall performance.\n",
    "\n",
    "# Uses models with different algorithms or trained in different subsets of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83446e41-aeda-4cb9-9ad8-a8e4b9ec46ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2.\n",
    "\n",
    "# Improved accuracy: Ensemble methods combine diverse models to enhance overall predictive performance.\n",
    "# Reduced Overfitting: Aggregating predictions from multiple models helps mitigate overfitting and increases generalization.\n",
    "# Robustness: Ensembles are more resilient to outliers and variations in the data.\n",
    "# Model Diversity: Combining different algorithms or subsets of data contributes to a more robust and versatile model.\n",
    "# Stability: Ensembles provide more stable and reliable predictions compared to individual models.\n",
    "# Common Types: Bagging, Boosting, Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34bd32d6-9a86-4b28-9cdd-5d4fca5002a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.\n",
    "# Bagging: Technique that combines predictions from multiple diverse models.\n",
    "# Data Sampling: Trains each model on a random subset of the training data.\n",
    "# Aggregation: Combines predicitons through averaging (regression) or voting (classification)\n",
    "# Reduces variance: Mitigates overfitting and increases stability by introducing diversity.\n",
    "# Example Algorithm: Random Forest is a popular bagging algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c678e418-09e7-4c75-bbeb-280724074d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4.\n",
    "# Boosting: Ensemble technique that combines weak learners to form a stron learner.\n",
    "# Sequential Training: Models are trained sequentially, and each corrects of its predecessor.\n",
    "# Weighted Importance: Assigns weights to data points based on their performance in previous models.\n",
    "# Examples: AdaBoost, Gradient Boosting Machines (GBM), XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cc6ce8d-8072-479f-b897-20f88082ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5.\n",
    "# Improved accuracy: Ensemble methods combine diverse models to enhance overall predictive performance.\n",
    "# Reduced Overfitting: Aggregating predictions from multiple models helps mitigate overfitting and increases generalization.\n",
    "# Robustness: Ensembles are more resilient to outliers and variations in the data.\n",
    "# Model Diversity: Combining different algorithms or subsets of data contributes to a more robust and versatile model.\n",
    "# Stability: Ensembles provide more stable and reliable predictions compared to individual models.\n",
    "# State-of-the-Art Performance: Widely used in competitions and real-world applications to achieve top-tier results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2f7f65a-7b99-464c-ab75-2289a3d50e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6.\n",
    "# Not always superior: Enseemble techniques may not always outperform individual models.\n",
    "# Data Suitability: Performance depends on the dataset; ensembles excel when there is diversity among models.\n",
    "# Computational Cost: Ensembles can be computationally expensive compared to single models.\n",
    "# Task Complexity: For simple tasks, the overhead of ensembles may not justify the marinal gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3219e8b3-80fd-478c-8d50-e4c8683d3040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7.\n",
    "# Data Resampling: Repeatedly draw random samples with replacement from the original dataset.\n",
    "# Statistic Computation: Calculate the desired statistic (mean, median, etc.) for each bootstrap sample.\n",
    "# Percentile Method: Use percentiles of the computed statistics to construct the confidence interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "044933e2-5981-4a4e-b42d-70f5b95745e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8.\n",
    "\n",
    "# Bootstrap Process:\n",
    "\n",
    "# Data Resampling: Generate multiple samples by randomly selecting data points with replacement from the original dataset.\n",
    "# Sample Size: Each bootstrap sample has the same size as the original dataset.\n",
    "# Statistic Computation: Calculate the desired statistic (mean, median, etc.) for each bootstrap sample.\n",
    "# Statistic Distribution: Create a distribution of the computed statistics from all bootstrap samples.\n",
    "# Confidence Interval: Construct a confidence interval using percentiles of the statistic distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81c1f2d5-6f5a-4bd6-a129-f5761b9b4a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52fc538d-5e63-42bf-ab6b-16584cd1b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c51b5cbe-bd6b-4d03-80ac-453fcb787884",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sample = np.random.normal(loc=15, scale=2, size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31067241-2cd6-4924-bb6f-a891558d9fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bootstraps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9bb4c3f-a2d6-4819-a7cc-805ef165e45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_means = [np.mean(np.random.choice(original_sample, size=50, replace=True))for _ in range(num_bootstraps)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dab09901-fdbd-40de-a239-fb319eb43876",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04be4b92-1fc3-4b13-ac36-e4388336253a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval for Mean Height: [14.88978179 15.81322555]\n"
     ]
    }
   ],
   "source": [
    "print(\"95% Confidence Interval for Mean Height:\", confidence_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5c70d5-8b7c-43e0-95d4-9a95855f34b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
